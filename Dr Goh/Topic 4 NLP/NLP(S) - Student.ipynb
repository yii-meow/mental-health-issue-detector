{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS-Uz6G-qcQa"
   },
   "source": [
    "# <font color=\"maroon\"> 1.0 NLP Toolkits and Preprocessing Techniques </font>\n",
    "\n",
    "### NLP Toolkits\n",
    "1.Python libraries for natural language processing\n",
    "\n",
    "### Text Preprocessing Techniques\n",
    "1.Converting text to a meaningful format for analysis<br>\n",
    "2.Preprocessing and cleaning text\n",
    "\n",
    "### The Top 10 NLP Tools\n",
    "1.MonkeyLearn | NLP made simple <br>\n",
    "2.Aylien | Leveraging news content with NLP<br>\n",
    "3.IBM Watson | A pioneer AI platform for businesses<br>\n",
    "4.Google Cloud NLP API | Google technology applied to NLP<br>\n",
    "5.Amazon Comprehend | An AWS service to get insights from text<br>\n",
    "<font color=\"red\">6.NLTK | The most popular Python library<br> </font>\n",
    "7.Stanford Core NLP | Stanfordâ€™s fast and robust toolkit<br>\n",
    "<font color=\"red\">8.TextBlob | An intuitive interface for NLTK<br></font>\n",
    "9.SpaCy | Super-fast library for advanced NLP tasks<br>\n",
    "10.GenSim | State-of-the-art topic modeling<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zrizWhzqcQd"
   },
   "source": [
    "## How to Install NLTK?\n",
    "\n",
    "### Method (i) Command Line\n",
    "pip install nlt\n",
    "\n",
    "### Method (ii) Jupyter Notebook\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "### Method (iii) Anaconda navigator (Environment)\n",
    "![NLTK.png](attachment:NLTK.png)\n",
    "\n",
    "### Method (iv) Download Package and Place into Site-package directory\n",
    "Install nltk toolkit from https://sourceforge.net/projects/nltk/<br>\n",
    "![](https://imgur.com/yRQpekK.png)\n",
    "\n",
    "Locate the package into site-package directory <br>\n",
    "(to find the path:<br> import site <br>site.getsitepackages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjoXXSJHqcQe"
   },
   "source": [
    "## Sample Text Data\n",
    "\n",
    "Consider this sentence:\n",
    "**Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers) from the\n",
    "store. Should I pick up some black-eyed peas as well?**\n",
    "\n",
    "Text data is messy. To analyze this data, we need to preprocess the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14418,
     "status": "ok",
     "timestamp": 1709603322113,
     "user": {
      "displayName": "CHING PANG GOH",
      "userId": "03150219032412111071"
     },
     "user_tz": -480
    },
    "id": "r2eHgJwxS7vr",
    "outputId": "d1baa748-269c-485c-cda7-54895abc05fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yikso\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yikso\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yikso\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\yikso\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yikso\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yikso\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkvL3jUVqcQf"
   },
   "source": [
    "![](https://i.imgur.com/pt5p6Hb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSr3t_VaqcQf"
   },
   "source": [
    "# Code: Tokenization (Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jAN6R4XsqcQg",
    "outputId": "e9468d26-25b9-4e50-bc89-4a5459730ca1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yikso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.', 'Smith', '!', 'I', 'am', 'going', 'to', 'buy', 'some', 'vegetables', '(', '3', 'tomatoes', 'and', '3', 'cucumbers', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
    "from the store. Should I pick up some black-eyed peas as well?'''\n",
    "\n",
    "print(word_tokenize(my_text)) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3JyUE6VqcQj"
   },
   "source": [
    "# Code: Tokenization (Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j20Ku-8fqcQj",
    "outputId": "91fea408-0b21-4b38-f7bb-7141634354ab",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Mr. Smith!', 'I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\\nfrom the store.', 'Should I pick up some black-eyed peas as well?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
    "from the store. Should I pick up some black-eyed peas as well?'''\n",
    "\n",
    "print(sent_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8x_Frq2qcQk"
   },
   "source": [
    "![](https://i.imgur.com/3L6x92C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TMJl3S1qcQk"
   },
   "source": [
    "# Code: Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Oo4_cwhqqcQk",
    "outputId": "e4365e30-cdf4-414d-af88-5d31d83af5c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Mr Smith I am going to buy some vegetables 3 tomatoes and 3 cucumbers\\nfrom the store Should I pick up some blackeyed peas as well'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Regular expression library\n",
    "import string\n",
    "#Replace punctuations with a white space\n",
    "#clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "#clean_text\n",
    "s = re.sub('[^\\w\\s]','',my_text)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlejBNv_qcQl"
   },
   "source": [
    "# Code: Make All Text Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "K8KrXfx6qcQl",
    "outputId": "4689390c-a561-459b-d50d-f2c161dc7119"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr smith i am going to buy some vegetables 3 tomatoes and 3 cucumbers\\nfrom the store should i pick up some blackeyed peas as well'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = s.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkHCMNHcqcQl"
   },
   "source": [
    "# Code: Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mKm0X8Q1qcQm",
    "outputId": "553cfadb-097a-427b-8ddd-bb4be893e0ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr smith i am going to buy some vegetables  tomatoes and  cucumbers\\nfrom the store should i pick up some blackeyed peas as well'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\d', '', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eHsAXtRqcQm"
   },
   "source": [
    "# <font color='blue'>Preprocessing: Stop Words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUxW6DbyqcQm"
   },
   "source": [
    "![](https://i.imgur.com/T5RJXrX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1prkKAOqcQm"
   },
   "source": [
    "# Code: Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHdXIIJTqcQm"
   },
   "source": [
    "from nltk.corpus import stopwords <br>\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp2xTEuIqcQm"
   },
   "source": [
    "# Code: Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJMaZx-PqcQn"
   },
   "source": [
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "juvbFllXqcQn",
    "outputId": "b2412719-6813-4b2f-fae5-c72ed0736e8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yikso\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>buy</th>\n",
       "      <th>cucumbers</th>\n",
       "      <th>eyed</th>\n",
       "      <th>going</th>\n",
       "      <th>hi</th>\n",
       "      <th>mr</th>\n",
       "      <th>peas</th>\n",
       "      <th>pick</th>\n",
       "      <th>smith</th>\n",
       "      <th>store</th>\n",
       "      <th>tomatoes</th>\n",
       "      <th>vegetables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  buy  cucumbers  eyed  going  hi  mr  peas  pick  smith  store  \\\n",
       "0      1    1          1     1      1   1   1     1     1      1      1   \n",
       "\n",
       "   tomatoes  vegetables  \n",
       "0         1           1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "my_text = [\"Hi Mr. Smith! Iâ€™m going to buy some vegetables \\\n",
    "(3 tomatoes and 3 cucumbers from the store. Should I pick up some black-eyed peas as well?\"]\n",
    "\n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(my_text)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "#Reference: https://www.geeksforgeeks.org/difference-between-pandas-vs-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b--7CcpRqcQn"
   },
   "source": [
    "![](https://i.imgur.com/9qllh8j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dQn6mMMqcQn"
   },
   "source": [
    "# Code: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nKAuswb1qcQn",
    "outputId": "ebbc173c-f339-4e41-f492-d1ed7188af0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive:driv\n",
      "drives:driv\n",
      "driver:driv\n",
      "drivers:driv\n",
      "driven:driv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Try some stems\n",
    "print('drive:{}'.format(stemmer.stem('drive')))\n",
    "print('drives:{}'.format(stemmer.stem('drives')))\n",
    "print('driver:{}'.format(stemmer.stem('driver')))\n",
    "print('drivers:{}'.format(stemmer.stem('drivers')))\n",
    "print('driven:{}'.format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-1ldPaxqcQo"
   },
   "source": [
    "# Code: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDzNQEbAqcQo",
    "outputId": "adef6303-73f1-4235-8702-3e4cf04206f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "input_str=\"been had done languages cities mice\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Vv-d7eOqcQo"
   },
   "source": [
    "![](https://i.imgur.com/8edVsCR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUuRIUniqcQu"
   },
   "source": [
    "# Code: Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO3XPXQ8qcQu",
    "outputId": "63749005-7f8f-456b-df32-1d9ca8389fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'NNP'), ('Smith', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "print(tokens)\n",
    "\n",
    "#Reference:https://pythonspot.com/nltk-speech-tagging/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJFAsY0PqcQu"
   },
   "source": [
    "![ ](https://imgur.com/UOlnpKT.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQePPfDmqcQv"
   },
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1gcCJ6tqcQv"
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "print (entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppZjmyJaqcQv"
   },
   "source": [
    "# <font color=\"blue\"> Prepocessing: Compound Term Extraction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6wAKEJdqcQw"
   },
   "source": [
    "![](https://i.imgur.com/q1WuWai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixuUBEN-qcQw"
   },
   "source": [
    "# Code: Compound Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTrNBHGaqcQw"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS6oOq8SqcQw"
   },
   "source": [
    "![](https://i.imgur.com/HpgLFOT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf9F1PGlqcQw"
   },
   "source": [
    "# Basic Pandas Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPMqo63hqcQx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(6,4))\n",
    "data = pd.read_csv('cookie_reviews.csv')\n",
    "#data\n",
    "\n",
    "#Selecting top and bottom rows:\n",
    "#Returns the first n rows.\n",
    "first = data\n",
    "s=first.head()\n",
    "#Returns the last n rows.\n",
    "first = data\n",
    "e=first.tail()\n",
    "\n",
    "print(s)\n",
    "print(e)\n",
    "\n",
    "#Selecting columns:\n",
    "#data['column_name']\n",
    "#or data.column_name\n",
    "col_names=data.columns\n",
    "print(\"\\n column names = \",col_names)\n",
    "print (\"\\n \\n\")\n",
    "#Selecting by indexer:\n",
    "data.iloc[0] #- first row of data frame\n",
    "#data.iloc[-1] #- last row of data frame\n",
    "#data.iloc[:,0] #- first column of data frame\n",
    "#data.iloc[:,-1] #- last column of data frame\n",
    "#Data.iloc[0,1] #â€“ first row, second column of the dataframe\n",
    "#data.iloc[0:4, 3:5] # first 4 rows and 3rd, 4th, 5th columns of data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSGCfTFcqcQx"
   },
   "source": [
    "![](https://i.imgur.com/w9gWcfX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehHfaGRTqcQx"
   },
   "outputs": [],
   "source": [
    "# Basic example\n",
    "square_me=lambda x: x*x\n",
    "\n",
    "my_numbers=[9, 3, 4, 100, 2, 1]\n",
    "my_numbers_squared = list(map(square_me, my_numbers))#map=applies a function to all the items in an input_list\n",
    "print(my_numbers_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnOUCKilqcQx"
   },
   "source": [
    "# <font color=red>Preprocessing Exercise </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx_4ft5PqcQy"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We will be using review data from Kaggle to practice preprocessing text data. The dataset contains user reviews for many products, but today we'll be focusing on the product in the dataset that had the most reviews - an oatmeal cookie.\n",
    "\n",
    "The following code will help you load in the data. If this is your first time using nltk, you'll to need to pip install it first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRLQtbgGqcQy"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuV0HV14qcQy"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cookie_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5sMkJuUqcQy"
   },
   "source": [
    "**Question 1:**\n",
    "\n",
    "Determine how many reviews there are in total.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UF1mOOSyqcQz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zifeU6_GqcQz"
   },
   "source": [
    "**Question 2:**\n",
    "    \n",
    "Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ypc5DzTzqcQz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFxNCKk3qcQz"
   },
   "source": [
    "**Question 3:**\n",
    "\n",
    "(a) Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqhHr5MkqcQz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMcmeUXWqcQ0"
   },
   "source": [
    "(b) Change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2xCgIaTqcQ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dEJu7pBqcQ0"
   },
   "source": [
    "(b) Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoUIaIzHqcQ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKM8tsqXqcQ0"
   },
   "source": [
    "# <font color=\"maroon\"> 2.0 Text Similarity Measures </font>\n",
    "\n",
    "- To measure distance between 2 string\n",
    "\n",
    "## 2.1 Applications\n",
    "- Information retrieval\n",
    "- Text classification\n",
    "- Document clustering\n",
    "- Topic Modeling\n",
    "- Matric decomposition\n",
    "\n",
    "To measure the word similarity, we use **<font color=\"blue\"> Levenshtein distance </font>**.\n",
    "- Minimum number of operations to get from one word to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs9_fC38qcQ0"
   },
   "source": [
    "![](https://i.imgur.com/FkdJmPi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0rrfJBfqcQ1",
    "outputId": "8b2d7b07-b163-4c05-f124-7be3c2abd961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\msi-g3250\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\msi-g3250\\anaconda3\\lib\\site-packages (from python-Levenshtein) (45.2.0.post20200210)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKnoanV_qcQ1",
    "outputId": "00906ede-7850-47a1-aa61-50c190ca0a49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance as lev\n",
    "lev('party', 'park')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYSDgUoJqcQ1"
   },
   "source": [
    "# TextBlob\n",
    "\n",
    "### Another toolkit other than NLTK\n",
    "\n",
    "- Wraps around NLTK and makes it easier to use\n",
    "\n",
    "### TextBlob capabilities\n",
    "\n",
    "- Tokenization\n",
    "- Parts of speech tagging\n",
    "- Sentiment analysis\n",
    "- Spell check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTHPF8usqcQ1"
   },
   "source": [
    "# TextBlob Demo: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAEs-i42qcQ2",
    "outputId": "2cedc593-4eda-4fd8-c24d-6654fa1b6c2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['We', \"'re\", 'moving', 'from', 'NLTK', 'to', 'TextBlob', 'How', 'fun'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
    "my_text.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKxnw5B-qcQ2"
   },
   "source": [
    "# TextBlob Demo: Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cC7dsGIRqcQ2",
    "outputId": "bd93b7ca-f37e-4143-8597-54269de8bd8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm great at spelling.\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"I'm graat at speling.\")\n",
    "print(blob.correct()) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rgx2pPHKqcQ2"
   },
   "source": [
    "<font color=\"blue\">\n",
    "## How does the correct function work?  <br>\n",
    "    \n",
    "- Calculates the Levenshtein distance between the word â€˜graatâ€™ and all words in its word list </br>\n",
    "- Of the words with the smallest Levenshtein distance, it outputs the most popular word </br></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBKID7hSqcQ2"
   },
   "source": [
    "# TextBlob Demo: Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZAWJ5P9qcQ3",
    "outputId": "1ce220d2-0ba6-4024-e6e5-e5844b27879c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John NNP\n",
      "hits VBZ\n",
      "the DT\n",
      "ball NN\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"John hits the ball.\")\n",
    "for words, tag in blob.tags:\n",
    " print (words, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXWFQfkuqcQ3"
   },
   "source": [
    "# TextBlob Demo: Language Detection and Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbSQjF3gqcQ3",
    "outputId": "40fe22b9-f7d5-4f1b-ae37-f27120cb6178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=TextBlob(\"Bonjour, comment allez-vous \")\n",
    "word.detect_language()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP2A4IIFqcQ3",
    "outputId": "005c0149-1a7e-488c-cdf2-0a70f8c114cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hello how are you\")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.translate(from_lang='fr', to ='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qntebPFmqcQ4"
   },
   "source": [
    "# Text Format for Analysis: Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWWe5yQ-qcQ4",
    "outputId": "dca3b03d-d244-4aba-8907-09ba2794e687"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  fun  is  one  second  the  third  this\n",
       "0    0         1      1    0   1    0       0    1      0     1\n",
       "1    0         1      0    0   1    0       1    1      0     1\n",
       "2    1         0      0    1   1    2       0    1      1     0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus =['This is the first document.', 'This is the second document.', 'And the third one. One is fun.'] #corpus=collection of teks\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "pd.DataFrame(X.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rk5hGWqsqcQ4"
   },
   "source": [
    "![](https://i.imgur.com/OQDeQlb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GKZW4BHqcQ4"
   },
   "source": [
    "# Document Similarity: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6r2l-g8qcQ4"
   },
   "source": [
    "![](https://i.imgur.com/PyirXsy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87NdBI6LqcQ4",
    "outputId": "2f12447e-f961-4805-d250-fb0fd70614a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>hot</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chai  chocolate  encoding  hot  latte  make  milk  sale  sun  today  \\\n",
       "0     0          0         0    1      0     0     0     0    1      0   \n",
       "1     0          1         0    1      0     1     1     0    0      0   \n",
       "2     0          0         1    1      0     0     0     0    0      0   \n",
       "3     1          0         0    0      1     0     1     0    0      0   \n",
       "4     0          0         0    1      0     0     0     1    0      1   \n",
       "\n",
       "   weather  \n",
       "0        1  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "# create the document-term matrix with count vectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8eaYOoZqcQ5"
   },
   "source": [
    "# Document Similarity: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txc9GRGrqcQ5",
    "outputId": "99a3ab34-d85a-4601-8782-0c5820059aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
      "[('The weather is hot under the sun', 'I make my hot chocolate with milk'), ('The weather is hot under the sun', 'One hot encoding'), ('The weather is hot under the sun', 'I will have a chai latte with milk'), ('The weather is hot under the sun', 'There is a hot sale today'), ('I make my hot chocolate with milk', 'One hot encoding'), ('I make my hot chocolate with milk', 'I will have a chai latte with milk'), ('I make my hot chocolate with milk', 'There is a hot sale today'), ('One hot encoding', 'I will have a chai latte with milk'), ('One hot encoding', 'There is a hot sale today'), ('I will have a chai latte with milk', 'There is a hot sale today')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[0.40824829]]),\n",
       "  ('The weather is hot under the sun', 'One hot encoding')),\n",
       " (array([[0.40824829]]), ('One hot encoding', 'There is a hot sale today')),\n",
       " (array([[0.35355339]]),\n",
       "  ('I make my hot chocolate with milk', 'One hot encoding')),\n",
       " (array([[0.33333333]]),\n",
       "  ('The weather is hot under the sun', 'There is a hot sale today')),\n",
       " (array([[0.28867513]]),\n",
       "  ('The weather is hot under the sun', 'I make my hot chocolate with milk')),\n",
       " (array([[0.28867513]]),\n",
       "  ('I make my hot chocolate with milk', 'There is a hot sale today')),\n",
       " (array([[0.28867513]]),\n",
       "  ('I make my hot chocolate with milk', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('The weather is hot under the sun', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]), ('One hot encoding', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('I will have a chai latte with milk', 'There is a hot sale today'))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
    "pairs = list(combinations(range(len(corpus)),2)) #sentence (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), .., (3,4))\n",
    "print(pairs)\n",
    "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
    "print (combos)\n",
    "\n",
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results, combos), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gunvfaHqcQ5",
    "outputId": "82427824-7673-42b0-f1fa-20473b32f01f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 4)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = list(combinations(range(5),2))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tPRR8cqqcQ6"
   },
   "source": [
    "![](https://i.imgur.com/jrfN6Jj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlbkiwMZqcQ6"
   },
   "source": [
    "![](https://i.imgur.com/BI8XP92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CfuItnrqcQ6"
   },
   "source": [
    "![](https://i.imgur.com/3IbfQXT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxWfpnJQqcQ6"
   },
   "source": [
    "![](https://i.imgur.com/pnNqzql.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DLg00m-qcQ6",
    "outputId": "edac0e02-9438-4826-e415-9230b4995e72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  fun  is  one  second  the  third  this\n",
       "0    0         1      1    0   1    0       0    1      0     1\n",
       "1    0         1      0    0   1    0       1    1      0     1\n",
       "2    1         0      0    1   1    2       0    1      1     0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus = ['This is the first document.',\n",
    "         'This is the second document.',\n",
    "         'And the third one. One is fun.']\n",
    "# original Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohRREtLfqcQ7",
    "outputId": "842c06bf-63fd-4d6d-8262-91924a18d641"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.72086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       and  document     first      fun        is      one    second  \\\n",
       "0  0.00000  0.450145  0.591887  0.00000  0.349578  0.00000  0.000000   \n",
       "1  0.00000  0.450145  0.000000  0.00000  0.349578  0.00000  0.591887   \n",
       "2  0.36043  0.000000  0.000000  0.36043  0.212876  0.72086  0.000000   \n",
       "\n",
       "        the    third      this  \n",
       "0  0.349578  0.00000  0.450145  \n",
       "1  0.349578  0.00000  0.450145  \n",
       "2  0.212876  0.36043  0.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv_tfidf = TfidfVectorizer()\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1cpWFXYqcQ7"
   },
   "source": [
    "![](https://i.imgur.com/xlJibKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vd0_0u9WqcQ7"
   },
   "source": [
    "## Document Similarity: Example with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QO7Lub0aqcQ7",
    "outputId": "2c293096-5847-4438-8b08-3b6f8f9fecb1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>hot</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580423</td>\n",
       "      <td>0.468282</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871247</td>\n",
       "      <td>0.490845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chai  chocolate  encoding       hot     latte      make      milk  \\\n",
       "0  0.000000   0.000000  0.000000  0.370086  0.000000  0.000000  0.000000   \n",
       "1  0.000000   0.580423  0.000000  0.327000  0.000000  0.580423  0.468282   \n",
       "2  0.000000   0.000000  0.871247  0.490845  0.000000  0.000000  0.000000   \n",
       "3  0.614189   0.000000  0.000000  0.000000  0.614189  0.000000  0.495524   \n",
       "4  0.000000   0.000000  0.000000  0.370086  0.000000  0.000000  0.000000   \n",
       "\n",
       "     sale     sun   today  weather  \n",
       "0  0.0000  0.6569  0.0000   0.6569  \n",
       "1  0.0000  0.0000  0.0000   0.0000  \n",
       "2  0.0000  0.0000  0.0000   0.0000  \n",
       "3  0.0000  0.0000  0.0000   0.0000  \n",
       "4  0.6569  0.0000  0.6569   0.0000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the document-term matrix with TF-IDF vectorizer\n",
    "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "dt_tfidf = pd.DataFrame(X_tfidf,columns=cv_tfidf.get_feature_names())\n",
    "dt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmJrEDvvqcQ8",
    "outputId": "14fec291-dc6d-4456-893c-d3ebd0948f07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.23204486]]),\n",
       "  ('I make my hot chocolate with milk', 'I will have a chai latte with milk')),\n",
       " (array([[0.18165505]]),\n",
       "  ('The weather is hot under the sun', 'One hot encoding')),\n",
       " (array([[0.18165505]]), ('One hot encoding', 'There is a hot sale today')),\n",
       " (array([[0.16050661]]),\n",
       "  ('I make my hot chocolate with milk', 'One hot encoding')),\n",
       " (array([[0.1369638]]),\n",
       "  ('The weather is hot under the sun', 'There is a hot sale today')),\n",
       " (array([[0.12101835]]),\n",
       "  ('The weather is hot under the sun', 'I make my hot chocolate with milk')),\n",
       " (array([[0.12101835]]),\n",
       "  ('I make my hot chocolate with milk', 'There is a hot sale today')),\n",
       " (array([[0.]]),\n",
       "  ('The weather is hot under the sun', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]), ('One hot encoding', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('I will have a chai latte with milk', 'There is a hot sale today'))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results_tfidf, combos), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoVOTPiBqcQ8"
   },
   "source": [
    "![](https://i.imgur.com/mj4J60v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvgj0NWCqcQ8"
   },
   "source": [
    "# <font color=red>Text Similarity Exercise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsfiIFfVqcQ8"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using a song lyric dataset from Kaggle to identify songs with similar lyrics. The data set contains artists, songs and lyrics for 55K+ songs, but today we will be focusing on songs by one group in particular - The Beatles.\n",
    "\n",
    "The following code will help you load in the data and get set up for this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDt2HCNFqcQ9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K953LjgoqcQ9"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('songdata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5Lu9K4PqcQ9"
   },
   "source": [
    "# Question 1\n",
    "\n",
    "Apply the following preprocessing steps:\n",
    "\n",
    "- Note the '\\n' (new line) characters in the lyrics. Remove them using regular expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVCj21-wqcQ9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2jifDdhqcQ9"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "(a) List all the rows with \"Imagine\" in the title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGqWax8aqcQ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHz_geBHqcQ-"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "(a) Extract the first line of lyric out from the first song.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8L0kiv9qcQ-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMF1LVkdqcQ-"
   },
   "source": [
    "(b) Find out the sentiment of the extracted lyric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQ7GjfIaqcQ-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
